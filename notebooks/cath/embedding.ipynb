{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperbolical Embedding of CATH Labels and Protein Sequences\n",
    "\n",
    "Hierarchy:\n",
    "[CATH Labels Hierarchy](http://www.cathdb.info/browse/tree)\n",
    "\n",
    "Embedding Method:\n",
    "[Poincar√© Embeddings for Learning Hierarchical Representations](https://papers.nips.cc/paper/2017/file/59dfa2df42d9e3d41f5b02bfc32229dd-Paper.pdf) & \n",
    "[Joint Learning of Hyperbolic Label Embeddings for Hierarchical Multi-label Classification: hierarchy embedding method](https://github.com/geoopt/geoopt/blob/master/examples/hyperbolic_multiclass_classification.ipynb)\n",
    "\n",
    "Loss function for embedding a known hierarchy:\n",
    "[Unit Ball Model for Embedding Hierarchical Structures in the Complex Hyperbolic Space](https://arxiv.org/pdf/2105.03966v3.pdf)\n",
    "\n",
    "Plots and manifold computations:\n",
    "[Geoopt library and usage example](https://github.com/geoopt/geoopt/blob/master/examples/hyperbolic_multiclass_classification.ipynb)\n",
    "\n",
    "Protein sequence embedding:\n",
    "[T5 model for protein sequences](https://huggingface.co/Rostlab/prot_t5_xl_half_uniref50-enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch; torch.manual_seed(1)\n",
    "import geoopt\n",
    "import math\n",
    "import tqdm\n",
    "import random\n",
    "import re\n",
    "\n",
    "import os, sys\n",
    "sys.path.append(os.path.join(os.getcwd(), '../..'))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "\n",
    "from src.cath.datasets import LabelDataset\n",
    "from src.utils import add_geodesic_grid\n",
    "from src.utils import choice\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using {}\".format(device))\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], dtype=torch.int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1, 2, 3])[2:1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21764657e1584d63bd4fbe4be0ead42e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title Load encoder-part of ProtT5 in half-precision. { display-mode: \"form\" }\n",
    "# Load ProtT5 in half-precision (more specifically: the encoder-part of ProtT5-XL-U50) \n",
    "def get_T5_model():\n",
    "    model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\")\n",
    "    model = model.to(device) # move model to GPU\n",
    "    model = model.eval() # set model to evaluation model\n",
    "    tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc', do_lower_case=False)\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A E T C X A X', 'S K T X P']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences_Example = [\"A E T C Z A O\",\"S K T Z P\"]\n",
    "sequences_Example = [re.sub(r\"[UZOB]\", \"X\", sequence) for sequence in sequences_Example]\n",
    "sequences_Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer.batch_encode_plus(seqs, add_special_tokens=True, padding=\"longest\")\n",
    "\n",
    "input_ids = torch.tensor(ids['input_ids'])\n",
    "attention_mask = torch.tensor(ids['attention_mask'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heps-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f9142d7ddd0beea42c6382d4784af463c15297f753233ce920e470f27cf3cc90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
